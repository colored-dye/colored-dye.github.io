@inproceedings{geiger2024finding,
  title        = {Finding alignments between interpretable causal variables and distributed neural representations},
  author       = {Geiger, Atticus and Wu, Zhengxuan and Potts, Christopher and Icard, Thomas and Goodman, Noah},
  booktitle    = {Causal Learning and Reasoning},
  pages        = {160--187},
  year         = {2024},
  organization = {PMLR}
}

@article{wu2023interpretability,
  title   = {Interpretability at scale: Identifying causal mechanisms in alpaca},
  author  = {Wu, Zhengxuan and Geiger, Atticus and Icard, Thomas and Potts, Christopher and Goodman, Noah},
  journal = {Advances in neural information processing systems},
  volume  = {36},
  pages   = {78205--78226},
  year    = {2023}
}

@article{mueller2025mib,
  title   = {MIB: A Mechanistic Interpretability Benchmark},
  author  = {Mueller, Aaron and Geiger, Atticus and Wiegreffe, Sarah and Arad, Dana and Arcuschin, Iv{\'a}n and Belfki, Adam and Chan, Yik Siu and Fiotto-Kaufman, Jaden and Haklay, Tal and Hanna, Michael and Huang, Jing and Gupta, Rohan and Nikankin, Yaniv and Orgad, Hadas and Prakash, Nikhil and Reusch, Anja and Sankaranarayanan, Aruna and Shao, Shun and Stolfo, Alessandro and Tutek, Martin and Zur, Amir and Bau, David and Belinkov, Yonatan},
  journal = {arXiv preprint arXiv:2504.13151},
  eprint  = {2504.13151},
  year    = {2025},
  note    = {Accepted to ICML 2025},
  url     = {https://arxiv.org/abs/2504.13151}
}

@article{geiger2025causal,
  title   = {Causal abstraction: A theoretical foundation for mechanistic interpretability},
  author  = {Geiger, Atticus and Ibeling, Duligur and Zur, Amir and Chaudhary, Maheep and Chauhan, Sonakshi and Huang, Jing and Arora, Aryaman and Wu, Zhengxuan and Goodman, Noah and Potts, Christopher and others},
  journal = {Journal of Machine Learning Research},
  volume  = {26},
  number  = {83},
  pages   = {1--64},
  year    = {2025}
}

@inproceedings{dai2024representational,
  title     = {Representational Analysis of Binding in Language Models},
  author    = {Dai, Qin  and
               Heinzerling, Benjamin  and
               Inui, Kentaro},
  editor    = {Al-Onaizan, Yaser  and
               Bansal, Mohit  and
               Chen, Yun-Nung},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2024},
  address   = {Miami, Florida, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.emnlp-main.967/},
  doi       = {10.18653/v1/2024.emnlp-main.967},
  pages     = {17468--17493},
  abstract  = {Entity tracking is essential for complex reasoning. To perform in-context entity tracking, language models (LMs) must bind an entity to its attribute (e.g., bind a container to its content) to recall attribute for a given entity. For example, given a context mentioning ``The coffee is in Box Z, the stone is in Box M, the map is in Box H'', to infer ``Box Z contains the coffee'' later, LMs must bind ``Box Z'' to ``coffee''. To explain the binding behaviour of LMs, existing research introduces a Binding ID mechanism and states that LMs use a abstract concept called Binding ID (BI) to internally mark entity-attribute pairs. However, they have not directly captured the BI information from entity activations. In this work, we provide a novel view of the Binding ID mechanism by localizing the BI information. Specifically, we discover that there exists a low-rank subspace in the hidden state (or activation) of LMs, that primarily encodes BIs. To identify this subspace, we take principle component analysis as our first attempt and it is empirically proven to be effective. Moreover, we also discover that when editing representations along directions in the subspace, LMs tend to bind a given entity to other attributes accordingly. For example, by patching activations along the BI encoding direction we can make the LM to infer ``Box Z contains the stone'' and ``Box Z contains the map''.}
}