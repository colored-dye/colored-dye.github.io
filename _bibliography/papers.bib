@article{bao2026faithful,
  title       = {Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions},
  author      = {Bao, Yuntai and Zhang, Xuhong and Chen, Jintao and Su, Ge and Cai, Yuxiang and Peng, Hao and Sun, Bing and Weng, Haiqin and Yan, Liu and Yin, Jianwei},
  journal     = {arXiv preprint arXiv:2602.05234},
  year        = {2026},
  abstract    = {Intervention-based model steering offers a lightweight and interpretable alternative to prompting and fine-tuning. However, by adapting strong optimization objectives from fine-tuning, current methods are susceptible to overfitting and often underperform, sometimes generating unnatural outputs. We hypothesize that this is because effective steering requires the faithful identification of internal model mechanisms, not the enforcement of external preferences. To this end, we build on the principles of distributed alignment search (DAS), the standard for causal variable localization, to propose a new steering method: Concept DAS (CDAS). While we adopt the core mechanism of DAS, distributed interchange intervention (DII), we introduce a novel distribution matching objective tailored for the steering task by aligning intervened output distributions with counterfactual distributions. CDAS differs from prior work in two main ways: first, it learns interventions via weak-supervised distribution matching rather than probability maximization; second, it uses DIIs that naturally enable bi-directional steering and allow steering factors to be derived from data, reducing the effort required for hyperparameter tuning and resulting in more faithful and stable control. On AxBench, a large-scale model steering benchmark, we show that CDAS does not always outperform preference-optimization methods but may benefit more from increased model scale. In two safety-related case studies, overriding refusal behaviors of safety-aligned models and neutralizing a chain-of-thought backdoor, CDAS achieves systematic steering while maintaining general model utility. These results indicate that CDAS is complementary to preference-optimization approaches and conditionally constitutes a robust approach to intervention-based model steering. Our code is available at \url{https://github.com/colored-dye/concept_das}.},
  selected    = {true},
  bibtex_show = {true},
  abbr        = {ICLR},
  pdf         = {https://arxiv.org/pdf/2602.05234}
}

@inproceedings{bao2025scalable,
  title       = {Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization},
  author      = {Bao, Yuntai and Zhang, Xuhong and Du, Tianyu and Zhao, Xinkui and Zong, Jiang and Peng, Hao and Yin, Jianwei},
  booktitle   = {Proceedings of the Thirty-Fourth International Joint Conference on
                 Artificial Intelligence, {IJCAI-25}},
  publisher   = {International Joint Conferences on Artificial Intelligence Organization},
  editor      = {James Kwok},
  pages       = {8022--8030},
  year        = {2025},
  month       = {8},
  note        = {Main Track},
  doi         = {10.24963/ijcai.2025/892},
  url         = {https://doi.org/10.24963/ijcai.2025/892},
  abstract    = {Pre-trained large language models (LLMs) are commonly fine-tuned to adapt to downstream tasks. Since the majority of knowledge is acquired during pre-training, attributing the predictions of fine-tuned LLMs to their pre-training data may provide valuable insights. Influence functions have been proposed as a means to explain model predictions based on training data. However, existing approaches fail to compute ``multi-stage'' influence and lack scalability to billion-scale LLMs. In this paper, we propose the multi-stage influence function to attribute the downstream predictions of fine-tuned LLMs to pre-training data under the full-parameter fine-tuning paradigm. To enhance the efficiency and practicality of our multi-stage influence function, we leverage Eigenvalue-corrected Kronecker-Factored (EK-FAC) parameterization for efficient approximation. Empirical results validate the superior scalability of EK-FAC approximation and the effectiveness of our multi-stage influence function. Additionally, case studies on a real-world LLM, dolly-v2-3b, demonstrate its interpretive power, with exemplars illustrating insights provided by multi-stage influence estimates.},
  selected    = {true},
  bibtex_show = {true},
  abbr        = {IJCAI},
  pdf         = {https://arxiv.org/pdf/2505.05017}
}

@inproceedings{bao2025probing,
  title       = {Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in {LLM}s Across Logical Transformations and Question Answering Tasks},
  author      = {Bao, Yuntai  and
                 Zhang, Xuhong  and
                 Du, Tianyu  and
                 Zhao, Xinkui  and
                 Feng, Zhengwen  and
                 Peng, Hao  and
                 Yin, Jianwei},
  editor      = {Che, Wanxiang  and
                 Nabende, Joyce  and
                 Shutova, Ekaterina  and
                 Pilehvar, Mohammad Taher},
  booktitle   = {Findings of the Association for Computational Linguistics: ACL 2025},
  month       = jul,
  year        = {2025},
  address     = {Vienna, Austria},
  publisher   = {Association for Computational Linguistics},
  url         = {https://aclanthology.org/2025.findings-acl.38/},
  doi         = {10.18653/v1/2025.findings-acl.38},
  pages       = {682--700},
  isbn        = {979-8-89176-256-5},
  abstract    = {Large language models (LLMs) are trained on extensive datasets that encapsulate substantial world knowledge. However, their outputs often include confidently stated inaccuracies. Earlier works suggest that LLMs encode truthfulness as a distinct linear feature, termed the ``truth direction'', which can classify truthfulness reliably. We address several open questions about the truth direction: (i) whether LLMs universally exhibit consistent truth directions; (ii) whether sophisticated probing techniques are necessary to identify truth directions; and (iii) how the truth direction generalizes across diverse contexts.Our findings reveal that not all LLMs exhibit consistent truth directions, with stronger representations observed in more capable models, particularly in the context of logical negation.Additionally, we demonstrate that truthfulness probes trained on declarative atomic statements can generalize effectively to logical transformations, question-answering tasks, in-context learning, and external knowledge sources.Finally, we explore the practical application of truthfulness probes in selective question-answering, illustrating their potential to improve user trust in LLM outputs.These results advance our understanding of truth directions and provide new insights into the internal representations of LLM beliefs.},
  selected    = {true},
  bibtex_show = {true},
  abbr        = {Findings of ACL},
  pdf         = {https://arxiv.org/pdf/2506.00823}
}

@inproceedings{wang2022songdriver,
  title     = {Songdriver: Real-time music accompaniment generation without logical latency nor exposure bias},
  author    = {Wang, Zihao and Zhang, Kejun and Wang, Yuxing and Zhang, Chen and Liang, Qihao and Yu, Pengfei and Feng, Yongsheng and Liu, Wenbo and Wang, Yikai and Bao, Yuntao and others},
  booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
  pages     = {1057--1067},
  year      = {2022},
  abstract  = {Real-time music accompaniment generation has a wide range of applications in the music industry, such as music education and live performances. However, automatic real-time music accompaniment generation is still understudied and often faces a trade-off between logical latency and exposure bias. In this paper, we propose SongDriver, a real-time music accompaniment generation system without logical latency nor exposure bias. Specifically, SongDriver divides one accompaniment generation task into two phases: 1) The arrangement phase, where a Transformer model first arranges chords for input melodies in real-time, and caches the chords for the next phase instead of playing them out. 2) The prediction phase, where a CRF model generates playable multi-track accompaniments for the coming melodies based on previously cached chords. With this two-phase strategy, SongDriver directly generates the accompaniment for the upcoming melody, achieving zero logical latency. Furthermore, when predicting chords for a timestep, SongDriver refers to the cached chords from the first phase rather than its previous predictions, which avoids the exposure bias problem. Since the input length is often constrained under real-time conditions, another potential problem is the loss of long-term sequential information. To make up for this disadvantage, we extract four musical features from a long-term music piece before the current time step as global information. In the experiment, we train SongDriver on some open-source datasets and an original Ã iSong Dataset built from Chinese-style modern pop music scores. The results show that SongDriver outperforms existing SOTA (state-of-the-art) models on both objective and subjective metrics, meanwhile significantly reducing the physical latency.},
  abbr      = {ACM MM},
  pdf       = {https://arxiv.org/pdf/2209.06054}
}
