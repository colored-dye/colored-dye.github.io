@article{bao2026faithful,
  title       = {Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions},
  author      = {Bao, Yuntai and Zhang, Xuhong and Chen, Jintao and Su, Ge and Cai, Yuxiang and Peng, Hao and Sun, Bing and Weng, Haiqin and Yan, Liu and Yin, Jianwei},
  journal     = {arXiv preprint arXiv:2602.05234},
  year        = {2026},
  selected    = {true},
  bibtex_show = {true}
}

@inproceedings{bao2025probing,
  title       = {Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in {LLM}s Across Logical Transformations and Question Answering Tasks},
  author      = {Bao, Yuntai  and
                 Zhang, Xuhong  and
                 Du, Tianyu  and
                 Zhao, Xinkui  and
                 Feng, Zhengwen  and
                 Peng, Hao  and
                 Yin, Jianwei},
  editor      = {Che, Wanxiang  and
                 Nabende, Joyce  and
                 Shutova, Ekaterina  and
                 Pilehvar, Mohammad Taher},
  booktitle   = {Findings of the Association for Computational Linguistics: ACL 2025},
  month       = jul,
  year        = {2025},
  address     = {Vienna, Austria},
  publisher   = {Association for Computational Linguistics},
  url         = {https://aclanthology.org/2025.findings-acl.38/},
  doi         = {10.18653/v1/2025.findings-acl.38},
  pages       = {682--700},
  isbn        = {979-8-89176-256-5},
  abstract    = {Large language models (LLMs) are trained on extensive datasets that encapsulate substantial world knowledge. However, their outputs often include confidently stated inaccuracies. Earlier works suggest that LLMs encode truthfulness as a distinct linear feature, termed the ``truth direction'', which can classify truthfulness reliably. We address several open questions about the truth direction: (i) whether LLMs universally exhibit consistent truth directions; (ii) whether sophisticated probing techniques are necessary to identify truth directions; and (iii) how the truth direction generalizes across diverse contexts.Our findings reveal that not all LLMs exhibit consistent truth directions, with stronger representations observed in more capable models, particularly in the context of logical negation.Additionally, we demonstrate that truthfulness probes trained on declarative atomic statements can generalize effectively to logical transformations, question-answering tasks, in-context learning, and external knowledge sources.Finally, we explore the practical application of truthfulness probes in selective question-answering, illustrating their potential to improve user trust in LLM outputs.These results advance our understanding of truth directions and provide new insights into the internal representations of LLM beliefs.},
  selected    = {true},
  bibtex_show = {true}
}

@inproceedings{bao2025scalable,
  title       = {Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization},
  author      = {Bao, Yuntai and Zhang, Xuhong and Du, Tianyu and Zhao, Xinkui and Zong, Jiang and Peng, Hao and Yin, Jianwei},
  booktitle   = {Proceedings of the Thirty-Fourth International Joint Conference on
                 Artificial Intelligence, {IJCAI-25}},
  publisher   = {International Joint Conferences on Artificial Intelligence Organization},
  editor      = {James Kwok},
  pages       = {8022--8030},
  year        = {2025},
  month       = {8},
  note        = {Main Track},
  doi         = {10.24963/ijcai.2025/892},
  url         = {https://doi.org/10.24963/ijcai.2025/892},
  selected    = {true},
  bibtex_show = {true}
}

@inproceedings{wang2022songdriver,
  title     = {Songdriver: Real-time music accompaniment generation without logical latency nor exposure bias},
  author    = {Wang, Zihao and Zhang, Kejun and Wang, Yuxing and Zhang, Chen and Liang, Qihao and Yu, Pengfei and Feng, Yongsheng and Liu, Wenbo and Wang, Yikai and Bao, Yuntao and others},
  booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
  pages     = {1057--1067},
  year      = {2022}
}
