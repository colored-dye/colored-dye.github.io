<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://colored-dye.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://colored-dye.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-03-01T08:32:30+00:00</updated><id>https://colored-dye.github.io/feed.xml</id><title type="html">colored-dye’s blog</title><subtitle>colored-dye&apos;s blog </subtitle><entry><title type="html">A Personal Review of *PO Algorithms</title><link href="https://colored-dye.github.io/blog/2026/grpo/" rel="alternate" type="text/html" title="A Personal Review of *PO Algorithms"/><published>2026-02-26T03:00:00+00:00</published><updated>2026-02-26T03:00:00+00:00</updated><id>https://colored-dye.github.io/blog/2026/grpo</id><content type="html" xml:base="https://colored-dye.github.io/blog/2026/grpo/"><![CDATA[<p>In this blog post, I provide an informal review of *PO (i.e. Policy Optimization or Preference Optimization) algorithms, especially including <em>Group Relative Policy Optimization (GRPO)</em><d-cite key="shao2024deepseekmath"></d-cite> and its variants, mainly for my own reference. And hopefully the content would benefit potential readers.</p> <p><strong>Roadmap.</strong></p> <ul> <li>The classic RL algorithm, <em>Proximal Policy Optimization (PPO)</em>;</li> <li>GRPO, which obviates the need for a value model as in PPO.</li> <li>DPO, which directly optimizes on preference pairs and removes the need for a reward model;</li> </ul> <h2 id="ppo">PPO</h2> <p>PPO is an actor-critic algorithm widely used for RL fine-tuning of LLMs<d-cite key="schulman2017proximal"></d-cite>. PPO maximizes the following objective:</p> \[\mathcal{J}_{\text{PPO}}(\theta) = \underset{\substack{q \sim P(Q), \\ \\ o \sim \pi_{\theta_{\text{old}}}(O \vert q)}}{\mathbb{E}} \frac{1}{\vert o \vert} \sum_{t=1}^{\vert o \vert} \min \left[ \frac{\pi_{\theta}(o_t \vert q, o_{\lt t})}{\pi_{\theta_{\text{old}}}(o_t \vert q, o_{\lt t})} A_t, \mathrm{clip} \left( \frac{\pi_{\theta}(o_t \vert q, o_{\lt t})}{\pi_{\theta_{\text{old}}}(o_t \vert q, o_{\lt t})}, 1-\epsilon, 1+\epsilon \right) A_t \right],\] <p>where $\pi_\theta$ and $\pi_{\theta_\text{old}}$ are current and old policy models, respectively; $q$ is question sampled from the question distribution and $o$ is outputs sampled from $\pi_{\theta_\text{old}}$; $\epsilon$ is clipping hyperparameter; $A_t$ is advantage that is computed via <em>Generalized Advantage Estimation (GAE)</em> based on rewards ${ r_{\geq t} }$ and learned value model $V_\psi$, where the value model is trained together with the policy model.</p> <p>The standard approach to obtain rewards is to add per-token KL penalty from a reference model:</p> <p>\begin{equation}\label{eq:ppo_reward} r_t = r_\phi(q, o_{\leq t}) - \beta \log \frac{\pi_{\theta}(o_t \vert q, o_{\lt t})}{\pi_{\theta_{\text{old}}}(o_t \vert q, o_{\lt t})}, \end{equation}</p> <p>where $r_\phi$ is reward model, $\phi_\text{ref}$ is the reference model (usually the initial policy) and $\beta$ controls the strength of KL penalty.</p> <h3 id="gae">GAE</h3> <h2 id="grpo">GRPO</h2> <p>PPO requires a separate value model that is usually the same size as the policy model, which brings substantial memory and computational costs. Additionally, the value function is the baseline in advantage computation; however, there is a mismatch between the value model and reward model: the value model is token-wise accurate whereas the reward model only assigns reward for the last token. Both concerns motivates GRPO, which removes the need for a token-wise value model.</p> <p>As is indicated by its name, GRPO samples a group of outputs from the old policy $\pi_{\theta_{\text{old}}}$: ${ o_1, o_2, \dots, o_G }$ where $G$ is group size. Then the average reward of the group is the baseline.</p> \[\mathcal{J}_{\text{GRPO}}(\theta) = \underset{\substack{q \sim P(Q), \\ \{ o_i \}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}}(O \vert q)}}{\mathbb{E}} \left[ \frac{1}{G} \sum_{i=1}^{G} \frac{1}{\vert o_i \vert } \sum_{t=1}^{\vert o_i \vert} \min \left[ \frac{\pi_{\theta}(o_{i,t} \vert q, o_{i,\lt t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \vert q, o_{i,\lt t})} \hat{A}_{i,t}, \mathrm{clip}\left( \frac{\pi_{\theta}(o_{i,t} \vert q, o_{i,\lt t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \vert q, o_{i,\lt t})}, 1-\epsilon, 1+\epsilon \right) \hat{A}_{i,t} \right] - \beta \mathbb{D}_{\text{KL}} \left( \pi_\theta \Vert \pi_{\text{ref}} \right) \right],\] <p>where $\hat{A}_{i,t}$ is the advantage computed based on relative rewards of each group. Additionally, different from the KL penalty term of Equation \eqref{eq:ppo_reward}, GRPO estimates KL divergence with the following unbiased estimator:</p> \[\mathbb{D}_{\text{KL}} \left(\pi_\theta \Vert \pi_\text{ref} \right) = \frac{\pi_{\theta_{\text{ref}}}(o_{i,t} \vert q, o_{i,\lt t})}{\pi_{\theta}(o_{i,t} \vert q, o_{i,\lt t})} - \log \frac{\pi_{\theta_{\text{ref}}}(o_{i,t} \vert q, o_{i,\lt t})}{\pi_{\theta}(o_{i,t} \vert q, o_{i,\lt t})} - 1.\] <p>The algorithm for iterative GRPO (<a href="#iterative_grpo">Figure 1</a>) shows how reference model and old policy model are iteratively updated. The old policy model is frequently updated for each sampled batch while the reference model is updated at larger intervals.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-26-grpo/iterative_grpo-480.webp 480w,/assets/img/2026-02-26-grpo/iterative_grpo-800.webp 800w,/assets/img/2026-02-26-grpo/iterative_grpo-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-26-grpo/iterative_grpo.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" id="iterative_grpo"> Figure 1. Iterative GRPO algorithm <d-cite key="shao2024deepseekmath"></d-cite>. </div> <p>The advantage $\hat{A}_{i,t}$ is computed via <strong>outcome supervision</strong> and <strong>process supervision</strong>. For outcome supervision, given a group of rewards $\mathbf{r} = \{ r_1, r_2, \dots, r_G \}$, the rewards are normalized within the group, such that the normalized reward is $\tilde{r}_i = \frac{r_i - \mathrm{mean}(\mathbf{r})}{\mathrm{std}(\mathbf{r})}$. The normalized reward is then assigned as the actual advantage $\hat{A}_{i,t} = \tilde{r}_i$.</p> <p>For process supervision, rewards are assigned to each step of outputs: $\mathbf{R} = \left\{ \left\{ r_1^{\text{index}(1)}, \dots, r_1^{\text{index}(K_1)} \right\}, \dots, \left\{ r_G^{\text{index}(1)}, \dots, r_G^{\text{index}(K_G)} \right\} \right\}$, where $\text{index}(j)$ is the ending token index of the $j$-th step and $K_j$ is the total number of steps in the $j$-th output. The rewards are normalized via: $\tilde{r}_i^{\text{index}(j)} = \frac{r_i^{\text{index}(j)} - \mathrm{mean}(\mathbf{R})}{\mathrm{std}(\mathbf{R})}$. The advantage of each output token is thus the sum of normalized rewards from the current step and all subsequent steps: $\hat{A}_{i,t} = \sum_{\text{index}(j)\geq t} \tilde{r}_i^{\text{index}(j)}$.</p> <h2 id="dapo">DAPO</h2> <h2 id="gspo">GSPO</h2> <h2 id="dpo">DPO</h2> <p><em>Direct Preference Optimization (DPO)</em><d-cite key="rafailov2023direct"></d-cite> hinges on the idea that the policy model and reference model jointly models rewards, thus no need for external reward models. This gives DPO several benefits over PPO. First, it removes the need to train a reward model and only needs to load the policy model and reference model in memory. Second, DPO does not require us to perform RL and is straightforward SFT, which is much simpler than RL.</p> <p>DPO directly minimizes the following loss function over a dataset of preference triplets, $\mathcal{D} = \{ (x, y_l, y_w) \}$, which are prompt, rejected response and chosen response, respectively:</p> \[\mathcal{L}(\pi_\theta, \pi_{\text{ref}}) = - \underset{(x, y_l, y_w) \sim \mathcal{D}}{\mathbb{E}} \left[ \log \left( \sigma \left( \beta \left( \log \frac{\pi_\theta(y_w \vert x)}{\pi_\text{ref}(y_w \vert x)} - \log \frac{\pi_\theta(y_l \vert x)}{\pi_\text{ref}(y_l \vert x)} \right) \right) \right) \right],\] <p>where $\beta$ is the coefficient that controls the strength of reference model constraint. The reward is $\hat{r}_\theta(x,y) = \beta \log \frac{\pi_\theta(y \vert x)}{\pi_\text{ref}(y \vert x)}$, which is implicitly defined by both the policy model and the reference model.</p> <h2 id="summary">Summary</h2> <table> <thead> <tr> <th>Method</th> <th>Token/sequence -level</th> <th>Online/offline</th> </tr> </thead> <tbody> <tr> <td>PPO</td> <td>Token</td> <td>Online</td> </tr> <tr> <td>GRPO</td> <td>Token</td> <td>Online</td> </tr> <tr> <td>DAPO</td> <td>Token</td> <td>Online</td> </tr> <tr> <td>GSPO</td> <td>Sequence</td> <td>Online</td> </tr> <tr> <td>DPO</td> <td>Token</td> <td>Offline</td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="tech"/><category term="RL"/><category term="LLM"/><summary type="html"><![CDATA[a review of Policy Optimization or Preference Optimization algorithms.]]></summary></entry><entry><title type="html">Concept Distributed Alignment Search for Faithful Representation Steering</title><link href="https://colored-dye.github.io/blog/2026/concept-das/" rel="alternate" type="text/html" title="Concept Distributed Alignment Search for Faithful Representation Steering"/><published>2026-02-18T11:00:00+00:00</published><updated>2026-02-18T11:00:00+00:00</updated><id>https://colored-dye.github.io/blog/2026/concept-das</id><content type="html" xml:base="https://colored-dye.github.io/blog/2026/concept-das/"><![CDATA[<p>In this blog post, I would like to extend upon our recent work, <a href="https://arxiv.org/abs/2602.05234">Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions</a>, especially regarding the conceptual nature of our method, <em>Concept Distributed Alignment Search (CDAS)</em>.</p> <h2 id="early-exploration-and-misconceptiontheoretical-discussions">Early exploration and misconception–theoretical discussions</h2> <p>In early 2025, I was deeply intrigued by the causal abstraction branch of mechanistic interpretability and was working on improving <em>Distributed Alignment Search (DAS)</em> <d-cite key="geiger2024finding,wu2023interpretability,geiger2025causal"></d-cite>, such that the resulting causal abstraction technique is able to learn from probabilistic intricacies. This is motivated by the fact that DAS only learns subspace projections using discrete labels and does not fully utilize the probabilistic information of the target labels. Suppose we are trying to localize the fact retrieval feature. Given a prompt in a QA task, <code class="language-plaintext highlighter-rouge">New York is in the country of</code>, multiple responses could be considered factually correct: <code class="language-plaintext highlighter-rouge">the U.S.</code>, <code class="language-plaintext highlighter-rouge">the United States</code>, <code class="language-plaintext highlighter-rouge">America</code>. By setting the answer to be strictly <code class="language-plaintext highlighter-rouge">U.S.</code> under greedy decoding might deviate from the model’s inherent tendencies since the model might prefer a different but semantically similar answer. By explicitly incorporating probabilities in the training objective of causal abstraction methods, we might be able to utilize the curated constant labels in a manner that is more faithful to the model of interest, without sampling labels from the target model and filtering for useful ones in a model-specific manner.</p> <p>We initially submitted the paper to NeurIPS 2025. However, our discussions with the reviewers made us aware of a fundamental mistake regarding the conceptual nature of our method: <strong>CDAS should be positioned as a steering method, not a causal variable localization method</strong>. More specifically, CDAS is dedicated for a subset of causal variables: those directly related to outputs or properties of outputs, e.g., output tokens and output-oriented concepts. These variables are usually leaf nodes of causal graphs or single parents of leaf nodes (e.g., <code class="language-plaintext highlighter-rouge">Y, Z</code> when the causal graph is a linear chain <code class="language-plaintext highlighter-rouge">X -&gt; Y -&gt; Z</code> or <code class="language-plaintext highlighter-rouge">Y, Z</code> when the graph is <code class="language-plaintext highlighter-rouge">X1 -&gt; Y, X2 -&gt; Y, Y -&gt; Z</code>). The practical implication is that CDAS fails to accomplish general-purpose causal abstraction like DAS.</p> <p>We use the case of multiple-choice task to help readers understand. The high-level causal model of multiple-choice tasks, $\mathcal{H}$ (shown in <a href="#mcqa_causal_model">Figure 1</a>) defines two important causal variables: $X_\text{Order}$ (position of the answer) and $O_\text{Answer}$ (answer token). According to Mueller et al. <d-cite key="mueller2025mib"></d-cite>, this is driven by the hypothesis that an LM accomplishes multiple-choice tasks in two steps with binding mechanism <d-cite key="dai2024representational"></d-cite>: it computes the index for its answer before retrieving the choice letter from the prompt with the index.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa_causal_model-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa_causal_model-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa_causal_model-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa_causal_model.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" id="mcqa_causal_model"> Figure 1. High-level causal model $\mathcal{H}$ of multiple-choice tasks. </div> <p>Let base inputs be $b$, a question prompt with choices <code class="language-plaintext highlighter-rouge">A, B, C, D</code>, and the correct choice letter is $y^b = \verb|C|$, then the choice index is 2. Let counterfactual inputs be $c$ with choices <code class="language-plaintext highlighter-rouge">E, F, G, H</code> and the correct choice letter is $y^c = \verb|F|$, then the choice index is 1. $b,c$ are essentially the same question, except that $c$ shuffles the order of choices and replaces choice letters. After interchange intervention on the $X_\text{Order}$ variable, the intervened output has the same choice index 1 as when inputs are $c$. Therefore intervening on base inputs $b$ yields an intervened counterfactual answer: $y^{b*}=\verb|B|$.</p> <p>Recall that positive term of the CDAS training objective is as follows:</p> \[D_{\Phi}^+ = \frac{1}{\vert y^{b*} \vert} \sum_{k=1}^{\vert y^{b*} \vert} D_{\mathrm{JS}}\left( \mathbf{p}_{\Phi} \left( \cdot \vert y^{b*}_{\lt k}, b; \mathbf{h} \leftarrow \Phi^{\mathrm{DII}}(c) \right) \big\| \mathbf{p} \left( \cdot \vert y^{b*} _{\lt k},c \right) \right),\] <p>where $D_{\mathrm{JS}}(\cdot \Vert \cdot)$ is Jensen-Shannon divergence.</p> <p>The problem is that, when conditioned with counterfactual inputs $c$, the un-intervened probabilities on intervened counterfactual labels $y^{b*}$, i.e. $p(y^{b*} \vert c)$, is low since $y^{b*} \neq y^c$. As a result, the intervened counterfactual label does not provide sufficient signal to optimize for alignment and the resulting intervention does not correspond to features of the target causal variable. The cause of this problem is that this intervened counterfactual label is the <strong>composite</strong> of answer index and input prompt and it is not even a plausible answer given counterfactual inputs. In contrast, DAS does not suffer from this problem since the loss signal comes from constant external labels, not model-induced probability distributions.</p> <p>Acknowledging this problem, we treat the CDAS method as identifying features for output-oriented concepts that directly informs concept-based steering. To make this point clear, we also mention that CDAS is <em>not</em> a general-purpose causal abstraction method in the main body of our paper:</p> <blockquote> <p>Remark (CDAS is not causal variable localization). While CDAS draws inspiration from DAS, it should not be viewed as a causal variable localization method: DAS assumes access to a high-level algorithm with near-perfect supervision; whereas our goal is not to identify ground-truth causal variables, but to find useful features that enable faithful steering. Thus, CDAS is best understood as a steering method motivated by causal variable localization principles.</p> </blockquote> <h2 id="cdas-for-causal-abstractionan-empirical-analysis">CDAS for causal abstraction?–an empirical analysis</h2> <p><strong>Benchmark dataset and metric.</strong> I tested CDAS on the causal variable localization track of <em>Mechanistic Interpretability Benchmark (MIB)</em> <d-cite key="mueller2025mib"></d-cite>. The target model is Gemma2-2B. We study three tasks: two multiple-choice datasets, MCQA and ARC, as well as two-digit addition. For the multiple-choice tasks, I conduct causal variable localization regarding the causal variables $X_\text{Order}$ and $O_\text{Answer}$. For the two-digit addition task (high-level causal hypothesis in <a href="#addition_causal_model">Figure 9</a>), I study $X_\text{Carry}$, the carry value of the “carry-the-one” algorithm that LMs are assumed to implement.</p> <p>The dataset consists of three subsets, corresponding to three types of counterfactuals: <code class="language-plaintext highlighter-rouge">answerPosition</code> (only change the orders of choices), <code class="language-plaintext highlighter-rouge">randomLetter</code> (only change the choice letters) and <code class="language-plaintext highlighter-rouge">answerPosition_randomLetter</code> (change both choice orders and letters). Examples of these counterfactuals are shown in <a href="#counterfactual_dataset">Figure 2</a>.</p> <p>Intervention positions include the last token (<code class="language-plaintext highlighter-rouge">last_token</code>) and the choice letter of the correct answer (<code class="language-plaintext highlighter-rouge">correct_symbol</code>).</p> <p>The metric is <em>interchange intervention accuracy (IIA)</em>. We now formulate this metric according to <d-cite key="mueller2025mib"></d-cite>. Given base and counterfactual inputs $(b, c)$, the interchange intervention $\mathcal{H}_{X \leftarrow \mathrm{Get}(\mathcal{H}(c), X)}(b)$ runs $\mathcal{H}$ on base input $b$ while fixing the variable $X$ to the value it takes when $\mathcal{H}$ is run on a counterfactual input $c$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/counterfactual_dataset-480.webp 480w,/assets/img/2026-02-18-concept-das/counterfactual_dataset-800.webp 800w,/assets/img/2026-02-18-concept-das/counterfactual_dataset-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/counterfactual_dataset.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" id="counterfactual_dataset"> Figure 2. Counterfactuals for the multiple-choice ARC task (taken from <d-cite key="mueller2025mib"></d-cite>). </div> <div class="grid-container"> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_answerPosition_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_answerPosition_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_answerPosition_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_answerPosition_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_randomLetter_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_randomLetter_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_randomLetter_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_randomLetter_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_average_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_average_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_average_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer/heatmap_average_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" id="results_cdas_answer"> Figure 3. IIA results regarding $O_\text{Answer}$ on MCQA task with CDAS. </div> <div class="grid-container"> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_answerPosition_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_answerPosition_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_answerPosition_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_answerPosition_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_randomLetter_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_randomLetter_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_randomLetter_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_randomLetter_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_average_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_average_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_average_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer/heatmap_average_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" id="results_das_answer"> Figure 4. IIA results regarding $O_\text{Answer}$ on MCQA task with DAS (taken from <d-cite key="mueller2025mib"></d-cite>). </div> <div class="grid-container"> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_answerPosition_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_answerPosition_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_answerPosition_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_answerPosition_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_randomLetter_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_randomLetter_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_randomLetter_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_randomLetter_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_average_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_average_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_average_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer/heatmap_average_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" id="results_full_vector_answer"> Figure 5. IIA results regarding $O_\text{Answer}$ on MCQA task with full-vector intervention (taken from <d-cite key="mueller2025mib"></d-cite>). </div> <div class="grid-container"> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_answerPosition_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_answerPosition_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_answerPosition_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_answerPosition_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_randomLetter_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_randomLetter_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_randomLetter_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_randomLetter_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_average_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_average_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_average_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-cdas/answer_pointer/heatmap_average_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" id="results_cdas_answer_pointer"> Figure 6. IIA results regarding $X_\text{Order}$ on MCQA task with CDAS. </div> <div class="grid-container"> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_answerPosition_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_answerPosition_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_answerPosition_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_answerPosition_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_randomLetter_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_randomLetter_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_randomLetter_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_randomLetter_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_average_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_average_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_average_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-das/answer_pointer/heatmap_average_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" id="results_das_answer_pointer"> Figure 7. IIA results regarding $X_\text{Order}$ on MCQA task with DAS (taken from <d-cite key="mueller2025mib"></d-cite>). </div> <div class="grid-container"> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_answerPosition_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_answerPosition_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_answerPosition_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_answerPosition_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_randomLetter_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_randomLetter_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_randomLetter_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_randomLetter_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_answerPosition_randomLetter_test_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="image-item"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_average_4_answer_MCQA-480.webp 480w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_average_4_answer_MCQA-800.webp 800w,/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_average_4_answer_MCQA-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/mcqa-gemma2-full-vector/answer_pointer/heatmap_average_4_answer_MCQA.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" id="results_full_vector_answer_pointer"> Figure 8. IIA results regarding $X_\text{Order}$ on MCQA task with full-vector intervention (taken from <d-cite key="mueller2025mib"></d-cite>). </div> <table> <thead> <tr> <th>Method</th> <th style="text-align: center">$O_\text{Answer}$</th> <th style="text-align: center">$X_\text{Order}$</th> </tr> </thead> <tbody> <tr> <td>CDAS</td> <td style="text-align: center">89 (95)</td> <td style="text-align: center">63 (77)</td> </tr> <tr> <td>DAS$^*$</td> <td style="text-align: center">95 (97)</td> <td style="text-align: center">77 (93)</td> </tr> <tr> <td>DBM$^*$</td> <td style="text-align: center">84 (99)</td> <td style="text-align: center">63 (84)</td> </tr> <tr> <td>Full vector$^*$</td> <td style="text-align: center">61 (100)</td> <td style="text-align: center">44 (77)</td> </tr> </tbody> </table> <div class="caption" id="results_aggregate_mcqa"> Table 1. Aggregate IIA results on MCQA task. IIA of a single layer is averaged across intervention positions and counterfactuals. Results outside parentheses are averaged across all layers while results inside parentheses are highest results across all layers. Results with * are taken from <d-cite key="mueller2025mib"></d-cite>. </div> <table> <thead> <tr> <th>Method</th> <th style="text-align: center">$O_\text{Answer}$</th> <th style="text-align: center">$X_\text{Order}$</th> </tr> </thead> <tbody> <tr> <td>CDAS</td> <td style="text-align: center">93 (97)</td> <td style="text-align: center">42 (61)</td> </tr> <tr> <td>DAS$^*$</td> <td style="text-align: center">88 (94)</td> <td style="text-align: center">76 (88)</td> </tr> <tr> <td>DBM$^*$</td> <td style="text-align: center">82 (99)</td> <td style="text-align: center">63 (80)</td> </tr> <tr> <td>Full vector$^*$</td> <td style="text-align: center">63 (100)</td> <td style="text-align: center">43 (74)</td> </tr> </tbody> </table> <div class="caption" id="results_aggregate_arc"> Table 2. Aggregate IIA results on ARC task. IIA of a single layer is averaged across intervention positions and counterfactuals. Results outside parentheses are averaged across all layers while results inside parentheses are highest results across all layers. Results with * are taken from <d-cite key="mueller2025mib"></d-cite>. </div> <table> <thead> <tr> <th>Method</th> <th style="text-align: center">$X_\text{Carry}$</th> </tr> </thead> <tbody> <tr> <td>CDAS</td> <td style="text-align: center">27 (31)</td> </tr> <tr> <td>DAS$^*$</td> <td style="text-align: center">31 (35)</td> </tr> <tr> <td>DBM$^*$</td> <td style="text-align: center">32 (44)</td> </tr> <tr> <td>Full vector$^*$</td> <td style="text-align: center">29 (35)</td> </tr> </tbody> </table> <div class="caption" id="results_aggregate_addition"> Table 3. Aggregate IIA results on the two-digit addition task. IIA of a single layer is averaged across intervention positions and counterfactuals. Results outside parentheses are averaged across all layers while results inside parentheses are highest results across all layers. Results with * are taken from <d-cite key="mueller2025mib"></d-cite>. </div> <p><strong>Results.</strong> Layer-wise CDAS results are shown in <a href="#results_cdas_answer">Figure 3</a> and <a href="#results_cdas_answer_pointer">Figure 6</a>, while layer-wise DAS results are shown in <a href="#results_das_answer">Figure 4</a> and <a href="#results_das_answer">Figure 7</a> and layer-wise full-vector results are shown in <a href="#results_full_vector_answer">Figure 5</a> and <a href="#results_full_vector_answer">Figure 8</a>. Comparing Figure 3 and 4, we can see that CDAS and DAS display qualitatively similar layer-wise performance for $O_\text{Answer}$. However, CDAS often yields low IIAs for $X_\text{Order}$ except for the <code class="language-plaintext highlighter-rouge">answerPosition</code> counterfactual.</p> <p>Aggregate results are shown in <a href="#results_aggregate_mcqa">Table 1</a>, <a href="#results_aggregate_arc">Table 2</a> and <a href="#results_aggregate_addition">Table 3</a>. IIA averaged across all layers tells us about the robustness of a causal variable localization, whereas the highest IAA of an individual layer yields the best IIA result obtained through layer-wise search. On the two multiple-choice tasks, the averaged CDAS performance with respect to $O_\text{Answer}$ is on par with DAS. However, its average performance with respect to $X_\text{Order}$ and $X_\text{Carry}$ is only comparable to the unsupervised full-vector baseline. The underperformance result indicates that CDAS fails to identify useful features for $X_\text{Order}$ and $X_\text{Carry}$. Both the positive and negative empirical results support our previous analysis that CDAS is not useful for internal</p> <p><strong>Takeaway.</strong> CDAS can only be used to align neural representations with high-level variables directly related to output content or properties of outputs, <em>not</em> the internal causal variables of high-level causal models.</p> <h2 id="acknowledgement">Acknowledgement</h2> <p>This post was inspired by a conversation with Professor Yonatan Belinkov. His curiosity regarding CDAS’s performance on MIB helped clarify these limitations, and I’m grateful for the nudge to get these results out there.</p> <h2 id="appendix">Appendix</h2> <h3 id="high-level-causal-model">High-level causal model</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2026-02-18-concept-das/addition_causal_model-480.webp 480w,/assets/img/2026-02-18-concept-das/addition_causal_model-800.webp 800w,/assets/img/2026-02-18-concept-das/addition_causal_model-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2026-02-18-concept-das/addition_causal_model.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption" id="addition_causal_model"> Figure 9. High-level causal model of two-digit addition task. </div>]]></content><author><name></name></author><category term="tech"/><category term="steering"/><category term="LLM"/><summary type="html"><![CDATA[discussions regarding our recent work on faithful representation steering.]]></summary></entry></feed>